{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > 24/04/2020\n",
    "### > Gozde Orhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection, Turkish or English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is done in order to propose a machine learning system to classify words into their languages, namely Turkish or English.\n",
    "\n",
    "The project is executed for a case study where 2 csv files were given: Dictionary_Turkish and Dictionary_English. Unfortunately the files are not permitted to be shared publicly. Both csv files consist of 4 to 6 characters long well-written, genuine words in Turkish and English.\n",
    "\n",
    "The proposed system is an *SVM classifier* preeceded by a simple if-else statement. \n",
    "\n",
    "- First, if the string contains one of the following: q, w, x, the word is classified as English. \n",
    "- If not, then, the word is fed into to the classifier to be labelled. \n",
    "\n",
    "The system does a great job detecting English words however it is less successful to detect Turkish words. \n",
    "\n",
    "The classifier and its parameters are selected based on comparison of following classifiers: Decision Tree, Random Forest, AdaBoost, GradientBoosting and SVM in terms of *precision and recall score for Turkish class*. Top performed classifier was selected based on best F1 score of Turkish class, which was SVM with a polynomial kernel of degree 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #regular experession, imported to find if a string has whitespace\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE #imported to generate synthetic data for minority class\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Turkish dataset: 2235\n",
      "Number of words in English dataset: 21666\n"
     ]
    }
   ],
   "source": [
    "#Read csv files into pandas dataframe\n",
    "turkish_df = pd.read_csv('Dictionary_Turkish.csv', keep_default_na=False)\n",
    "english_df = pd.read_csv('Dictionary_English.csv', keep_default_na=False)\n",
    "\n",
    "#Number of words (rows) in datasets\n",
    "print('Number of words in Turkish dataset: '+ str(len(turkish_df)),\n",
    "      'Number of words in English dataset: '+ str(len(english_df)), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kelimeler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fasit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Kelimeler\n",
       "0      frak\n",
       "1     fasit"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview dataset\n",
    "turkish_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fillet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words\n",
       "0  flying\n",
       "1  fillet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview dataset\n",
    "english_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Column name of turkish_df changed for convenience\n",
    "turkish_df = turkish_df.rename(columns={'Kelimeler': 'Words'})\n",
    "\n",
    "#Words are changed to lowercase for consistency\n",
    "turkish_df['Words']=turkish_df['Words'].str.lower()\n",
    "english_df['Words']=english_df['Words'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Turkish dataset: 2235\n",
      "Number of words in English dataset: 21666\n"
     ]
    }
   ],
   "source": [
    "#Drop duplicate, if any\n",
    "turkish_df = turkish_df.drop_duplicates(subset='Words',keep='first', inplace=False)\n",
    "english_df = english_df.drop_duplicates(subset='Words',keep='first', inplace=False)\n",
    "\n",
    "#Number of words (rows) in datasets\n",
    "print('Number of words in Turkish dataset: '+ str(len(turkish_df)),\n",
    "      'Number of words in English dataset: '+ str(len(english_df)), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkish dataset: \n",
      "Words    0\n",
      "dtype: int64\n",
      "\n",
      "English dataset: \n",
      "Words    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Control missing data\n",
    "print('Turkish dataset: ', turkish_df.isnull().sum(), sep = '\\n')\n",
    "\n",
    "print()\n",
    "\n",
    "print('English dataset: ', english_df.isnull().sum(), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data preparation, assign language codes to words\n",
    "turkish_df.insert(1, \"Lang_Code\", \"0\")\n",
    "english_df.insert(1, \"Lang_Code\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkish dataset: \n",
      "count    2235.000000\n",
      "mean        5.268009\n",
      "std         0.709465\n",
      "min         4.000000\n",
      "25%         5.000000\n",
      "50%         5.000000\n",
      "75%         6.000000\n",
      "max         6.000000\n",
      "Name: Words, dtype: float64\n",
      "\n",
      "English dataset: \n",
      "count    21666.000000\n",
      "mean         5.416459\n",
      "std          0.726006\n",
      "min          4.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          6.000000\n",
      "max          6.000000\n",
      "Name: Words, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#See words' lengths in order to see if they are distinctive enough\n",
    "print('Turkish dataset: ', turkish_df[\"Words\"].apply(len).describe(), sep = '\\n')\n",
    "\n",
    "print()\n",
    "\n",
    "print('English dataset: ', english_df[\"Words\"].apply(len).describe(), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length feature was not distinctive because both corpus included words 4 to 6 letters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23767\n"
     ]
    }
   ],
   "source": [
    "#Combine datasets, create a single dataset consists of TR and ENG words\n",
    "combined_df = pd.concat([turkish_df, english_df])\n",
    "\n",
    "#Drop if duplicates exists - it existed but the TR ones are kept due to the fact that TR words are already less\n",
    "combined_df = combined_df.drop_duplicates(subset='Words', keep='first', inplace=False)\n",
    "\n",
    "#Shuffle data to avoid any bias may occur\n",
    "combined_df = combined_df.sample(frac=1)\n",
    "\n",
    "#Reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23756\n"
     ]
    }
   ],
   "source": [
    "#Drop strings containing whitespace (e.g. uc iki)\n",
    "for i in range(len(combined_df)):\n",
    "    st = combined_df.at[i,'Words']\n",
    "    if bool(re.search(r\"\\s\", st)) == True:\n",
    "        combined_df = combined_df.drop([i])\n",
    "\n",
    "#Reset index        \n",
    "combined_df = combined_df.reset_index(drop=True)  \n",
    "\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed words into classifiers, words were tranformed into vectors. Vectors are (26x6) long because maximum length of words is 6 and alphabet has 26 characters (Turkish characters such as ç, ö, ü, etc. are omitted). \n",
    "\n",
    "Example (parantheses are for illustrative purposes):\n",
    "\n",
    "- 'a' is represented as (**1**0000000000000000000000000)\n",
    "- 'masa' is represented as (00000000000**1**00000000000000)(**1**0000000000000000000000000)(00000000000000000**1**00000000)(**1**0000000000000000000000000)(000000000000000000000000000000)(000000000000000000000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Lang_Code</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jitney</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urnful</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>itch</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rose</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asemia</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words Lang_Code    0    1    2    3    4    5    6    7  ...  146  147  \\\n",
       "0  jitney         1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1  urnful         1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2    itch         1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3    rose         1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4  asemia         1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   148  149  150  151  152  153  154  155  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 158 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(combined_df)):   \n",
    "    \n",
    "    word = combined_df.at[i,'Words'] #get word\n",
    "    n = len(word)\n",
    "    vec = \" \"\n",
    "    \n",
    "    for j in range(n):\n",
    "        letter = word[j]\n",
    "        pos = ord(letter)-97 #get unicode\n",
    "        vector = (str(0)*pos) + str(1) + str(0)*(25-pos) #fill 1 and 0's for existing letters\n",
    "        vec = vec + vector #update empty string\n",
    "        \n",
    "    if n <= 6:\n",
    "        remaining = 6-n\n",
    "        vec = vec + str(0)*26*remaining #fill 0's if word is shorter than 6\n",
    "\n",
    "    result = [int(let) for let in str(vec) if let.isdigit()]\n",
    "    \n",
    "    for k in range(len(result)):\n",
    "        combined_df.at[i, k] = (result[k])\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18329\n",
       "0     1863\n",
       "Name: Lang_Code, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get digitized vector columns as features\n",
    "features = combined_df.iloc[:, 2:]\n",
    "\n",
    "#Get Lang_Code as target\n",
    "labels = combined_df.iloc[:, 1]\n",
    "\n",
    "#Split dataset - 85% training and %15 test\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = .15)\n",
    "\n",
    "y_train.value_counts() #THERE IS A HUGE IMBALANCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18329\n",
       "0    18329\n",
       "Name: Lang_Code, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In order to avoid bias as possible, over-sampling method SMOTE is utilized\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=95)\n",
    "x_bal, y_bal = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "y_bal.value_counts() #now the class balance is 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels\n",
    "Encoder = LabelEncoder()\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)\n",
    "y_bal = Encoder.fit_transform(y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(), \n",
    "               GradientBoostingClassifier(), SVC(C=1, kernel='poly', degree=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================== DecisionTreeClassifier ===================================\n",
      " \n",
      "Accuracy Score  ->  0.8866442199775533\n",
      "F1 Score with   ->  [0.44198895 0.93691443]\n",
      "Recall Score    ->  [0.44077135 0.93720712]\n",
      "Precision Score ->  [0.4432133  0.93662192]\n",
      " \n",
      "Confusion matrix: \n",
      "[[ 160  203]\n",
      " [ 201 3000]]\n",
      "=================================== RandomForestClassifier ===================================\n",
      " \n",
      "Accuracy Score  ->  0.9172278338945006\n",
      "F1 Score with   ->  [0.43809524 0.95532334]\n",
      "Recall Score    ->  [0.70121951 0.92764706]\n",
      "Precision Score ->  [0.31855956 0.98470184]\n",
      " \n",
      "Confusion matrix: \n",
      "[[ 115   49]\n",
      " [ 246 3154]]\n",
      "=================================== AdaBoostClassifier ===================================\n",
      " \n",
      "Accuracy Score  ->  0.7623456790123457\n",
      "F1 Score with   ->  [0.35589354 0.8542921 ]\n",
      "Recall Score    ->  [0.24528302 0.951341  ]\n",
      "Precision Score ->  [0.64819945 0.77521074]\n",
      " \n",
      "Confusion matrix: \n",
      "[[ 234  720]\n",
      " [ 127 2483]]\n",
      "=================================== GradientBoostingClassifier ===================================\n",
      " \n",
      "Accuracy Score  ->  0.819023569023569\n",
      "F1 Score with   ->  [0.38512869 0.89389702]\n",
      "Recall Score    ->  [0.29360465 0.94471488]\n",
      "Precision Score ->  [0.55955679 0.84826725]\n",
      " \n",
      "Confusion matrix: \n",
      "[[ 202  486]\n",
      " [ 159 2717]]\n",
      "=================================== SVC ===================================\n",
      " \n",
      "Accuracy Score  ->  0.877665544332211\n",
      "F1 Score with   ->  [0.50341686 0.93024   ]\n",
      "Recall Score    ->  [0.42746615 0.95405317]\n",
      "Precision Score ->  [0.61218837 0.90758664]\n",
      " \n",
      "Confusion matrix: \n",
      "[[ 221  296]\n",
      " [ 140 2907]]\n"
     ]
    }
   ],
   "source": [
    "for j in classifiers:\n",
    "    \n",
    "    name = j.__class__.__name__\n",
    "    print('=================================== ' + str(name) + ' ===================================')\n",
    "    \n",
    "    #Fit classifier\n",
    "    classifier = j\n",
    "    classifier.fit(x_bal,y_bal)\n",
    "\n",
    "    #Predict the labels on test set\n",
    "    predictions = classifier.predict(x_test)\n",
    "\n",
    "    #Get evaluation metrics and print\n",
    "    print(' ')\n",
    "    print('Accuracy Score  -> ',accuracy_score(predictions, y_test))\n",
    "    print('F1 Score with   -> ',f1_score(predictions, y_test, average=None))\n",
    "    print('Recall Score    -> ',recall_score(predictions, y_test, average=None))\n",
    "    print('Precision Score -> ',precision_score(predictions, y_test, average=None))\n",
    "    print(' ')\n",
    "    print('Confusion matrix: ', confusion_matrix(predictions, y_test), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since every classifier was successful classifying English words, mostly precision and recall score metrics for Turkish class were taken into consideration. SVM outperformed other classifiers with the average recall being ~42% and average precision ~60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed ML system is significantly successful identifying English words, which may be an indicator that if a larger Turkish corpus can be utilized, classifier would be enable to succesfully detect language. In addition, word-length constraint in vectorization is challenging since it limits the algorithm's ability to detect the language of longer words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn to test! Enjoy! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to predict:\n",
      "mavi\n",
      "English\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "kahve\n",
      "Turkish\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "cam\n",
      "Turkish\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "remote\n",
      "English\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "chair\n",
      "English\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "black\n",
      "English\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "sehpa\n",
      "English\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "kapi\n",
      "Turkish\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "guzel\n",
      "Turkish\n",
      "\n",
      "\n",
      "Enter word to predict:\n",
      "islev\n",
      "Turkish\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Interactive block of code enables users to enter their own words for algorithm to classify!\n",
    "\n",
    "#Allow users to enter 10 words, run code again if further testing required\n",
    "count = 0\n",
    "valid = True\n",
    "\n",
    "while valid==True:\n",
    "    if count==10:\n",
    "        valid = False\n",
    "    else:\n",
    "        word = input('Enter word to predict:\\n')\n",
    "\n",
    "        #Since our algorithm deals with max 6-length words, check length\n",
    "        if len(word) <= 6:\n",
    "            word = word.lower()\n",
    "            n = len(word)\n",
    "            vec = ''\n",
    "            chars = set('wxq')\n",
    "\n",
    "            if any((c in chars) for c in word): #if string contains w, x or q\n",
    "                print('English')\n",
    "                count+=1\n",
    "                \n",
    "            else:\n",
    "                #Vectorize\n",
    "                for j in range(n):\n",
    "                    letter = word[j]\n",
    "                    pos = ord(letter)-97 #get unicode\n",
    "                    vector = (str(0)*pos) + str(1) + str(0)*(25-pos) #fill 1 and 0's for existing letters\n",
    "                    vec = vec + vector #update empty string\n",
    "\n",
    "                if n <= 6:\n",
    "                    remaining = 6-n\n",
    "                    vec = vec + str(0)*26*remaining #fill 0's if word is shorter than 6\n",
    "\n",
    "                result = [int(let) for let in str(vec) if let.isdigit()]\n",
    "                result = (np.asarray(result)).reshape(1,-1)\n",
    "                \n",
    "                \n",
    "                prediction_rf = classifier.predict(result) #feed word into classifier\n",
    "\n",
    "                if (prediction_rf[0] == 0):\n",
    "                    print('Turkish')\n",
    "                else:\n",
    "                    print('English')\n",
    "            print('\\n')\n",
    "            count+=1\n",
    "        else:\n",
    "            count+=1\n",
    "            print('Word must be less than ' + str(7) + ' letters long')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
